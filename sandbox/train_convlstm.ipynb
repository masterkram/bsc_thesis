{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.ConvLSTM import ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'convlstm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdset\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mconvlstm\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvLSTM\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcnn\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvEncoder, ConvDecoder\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtest\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'convlstm'"
     ]
    }
   ],
   "source": [
    "import test\n",
    "\n",
    "\n",
    "#-------parameters----\n",
    "\n",
    "b_size     = 32\n",
    "\n",
    "hidden_dim = 64\n",
    "hidden_spt = 16\n",
    "\n",
    "lstm_dims = [64,64,64]\n",
    "\n",
    "teacher_forcing = True\n",
    "\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#----------create some dirs---------\n",
    "\n",
    "root_log_dir = './results_verylong_' + ('' if teacher_forcing else 'ntf_') + \\\n",
    "                                                    str(lstm_dims)[1:-1].replace(', ','x')\n",
    "\n",
    "exists = True\n",
    "while exists:\n",
    "    exists = os.path.exists(root_log_dir)\n",
    "    if not exists:\n",
    "        os.makedirs(root_log_dir)\n",
    "    else:\n",
    "        root_log_dir += '_'\n",
    "\n",
    "train_out_dir = root_log_dir + '/train'\n",
    "test_out_dir = root_log_dir + '/test'\n",
    "\n",
    "chkpnt_dir = root_log_dir + '/checkpoint'\n",
    "\n",
    "os.makedirs(train_out_dir)\n",
    "os.makedirs(test_out_dir)\n",
    "os.makedirs(chkpnt_dir)\n",
    "\n",
    "log_file = root_log_dir + '/out.log'\n",
    "f = open(log_file,\"w+\")\n",
    "f.close()\n",
    "\n",
    "#-------some methods---------\n",
    "\n",
    "def save_model(step, train_bce, test_bce):\n",
    "    new_dir =  chkpnt_dir + \"/{0:0>5}_iter|test_bce:{1:.4f}|train_bce:{2:.4f}\".format(step, test_bce, train_bce)\n",
    "    os.makedirs(new_dir)\n",
    "\n",
    "    torch.save(lstm_encoder.state_dict(), new_dir + '/lstm_encoder.msd')\n",
    "    torch.save(lstm_decoder.state_dict(), new_dir + '/lstm_decoder.msd')\n",
    "    torch.save(cnn_encoder.state_dict(), new_dir + '/cnn_encoder.msd')\n",
    "    torch.save(cnn_decoder.state_dict(), new_dir + '/cnn_decoder.msd')\n",
    "\n",
    "def get_sample_prob(step):\n",
    "    alpha = 2450#1150\n",
    "    beta  = 8000\n",
    "    return alpha / (alpha + np.exp((step + beta) / alpha))\n",
    "\n",
    "#----------data---------\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.from_numpy(x))\n",
    "    ])\n",
    "\n",
    "raw_data  = BouncingMnist(transform=transform)\n",
    "dloader   = torch.utils.data.DataLoader(raw_data, batch_size=b_size,\n",
    "                                      shuffle=True, drop_last=True, num_workers=4)\n",
    "\n",
    "tester = test.TesterMnist(out_dir=test_out_dir)\n",
    "\n",
    "#--------model---------\n",
    "\n",
    "cnn_encoder = ConvEncoder(hidden_dim)\n",
    "cnn_decoder = ConvDecoder(b_size=b_size,inp_dim=hidden_dim)\n",
    "\n",
    "cnn_encoder.cuda()\n",
    "cnn_decoder.cuda()\n",
    "\n",
    "lstm_encoder = ConvLSTM(\n",
    "                   input_size=(hidden_spt,hidden_spt),\n",
    "                   input_dim=hidden_dim,\n",
    "                   hidden_dim=lstm_dims,\n",
    "                   kernel_size=(3,3),\n",
    "                   num_layers=3,\n",
    "                   peephole=True,\n",
    "                   batchnorm=False,\n",
    "                   batch_first=True,\n",
    "                   activation=F.tanh\n",
    "                  )\n",
    "\n",
    "lstm_decoder = ConvLSTM(\n",
    "                   input_size=(hidden_spt,hidden_spt),\n",
    "                   input_dim=hidden_dim,\n",
    "                   hidden_dim=lstm_dims,\n",
    "                   kernel_size=(3,3),\n",
    "                   num_layers=3,\n",
    "                   peephole=True,\n",
    "                   batchnorm=False,\n",
    "                   batch_first=True,\n",
    "                   activation=F.tanh\n",
    "                  )\n",
    "\n",
    "lstm_encoder.cuda()\n",
    "lstm_decoder.cuda()\n",
    "\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "crit = nn.BCELoss()\n",
    "crit.cuda()\n",
    "\n",
    "\n",
    "params = list(cnn_encoder.parameters()) + list(cnn_decoder.parameters()) + \\\n",
    "         list(lstm_encoder.parameters()) + list(lstm_decoder.parameters())\n",
    "\n",
    "p_optimizer = optim.Adam(params)\n",
    "\n",
    "#--------train---------\n",
    "\n",
    "i = 0\n",
    "\n",
    "for e in range(100):\n",
    "    for _, batch in enumerate(dloader):\n",
    "        p_optimizer.zero_grad()\n",
    "\n",
    "        seqs = batch\n",
    "        nextf_raw = seqs[:,10:,:,:,:].cuda()\n",
    "\n",
    "        #----cnn encoder----\n",
    "\n",
    "        prevf_raw = seqs[:,:10,:,:,:].contiguous().view(-1,1,64,64).cuda()\n",
    "        prevf_enc = cnn_encoder(prevf_raw).view(b_size,10,hidden_dim,hidden_spt,hidden_spt)\n",
    "\n",
    "        if teacher_forcing:\n",
    "            cnn_encoder_out = cnn_encoder(seqs[:,10:,:,:,:].contiguous().view(-1,1,64,64).cuda())\n",
    "            nextf_enc       = cnn_encoder_out.view(b_size,10,hidden_dim,hidden_spt,hidden_spt)\n",
    "\n",
    "        #----lstm encoder---\n",
    "\n",
    "        hidden           = lstm_encoder.get_init_states(b_size)\n",
    "        _, encoder_state = lstm_encoder(prevf_enc, hidden)\n",
    "\n",
    "        #----lstm decoder---\n",
    "\n",
    "        sample_prob =  get_sample_prob(i) if teacher_forcing else 0\n",
    "        decoder_output_list = []\n",
    "        r_hist = []\n",
    "\n",
    "        for s in range(10):\n",
    "            if s == 0:\n",
    "                decoder_out, decoder_state = lstm_decoder(prevf_enc[:,-1:,:,:,:], encoder_state)\n",
    "            else:\n",
    "                r = np.random.rand()\n",
    "                r_hist.append(int(r > sample_prob)) #debug\n",
    "\n",
    "                if r > sample_prob:\n",
    "                    decoder_out, decoder_state = lstm_decoder(decoder_out, decoder_state)\n",
    "                else:\n",
    "                    decoder_out, decoder_state = lstm_decoder(nextf_enc[:,s-1:s,:,:,:], decoder_state)\n",
    "\n",
    "            decoder_output_list.append(decoder_out)\n",
    "\n",
    "        final_decoder_out = torch.cat(decoder_output_list, 1)\n",
    "\n",
    "        #----cnn decoder----\n",
    "\n",
    "        decoder_out_rs  = final_decoder_out.view(-1,hidden_dim,hidden_spt,hidden_spt)\n",
    "\n",
    "        cnn_decoder_out_raw = F.sigmoid(cnn_decoder(decoder_out_rs))\n",
    "        cnn_decoder_out     = cnn_decoder_out_raw.view(b_size,10,1,64,64)\n",
    "\n",
    "        #-----predictor loss----------\n",
    "\n",
    "        pred_loss = crit(cnn_decoder_out, nextf_raw)\n",
    "\n",
    "        total_loss = pred_loss\n",
    "        total_loss.backward()\n",
    "        p_optimizer.step()\n",
    "\n",
    "        #----ouputs---------\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            output_str = \" | Epoch: {0} | Iter: {1} |\\n\".format(e, i)\n",
    "            output_str += \"   TotalLoss:     {0:.6f}\\n\".format(total_loss.item())\n",
    "            output_str += \"-------------------------------------------\\n\"\n",
    "            output_str += \"   Sampling prob: {0:.3f}\\n\".format(sample_prob)\n",
    "            output_str += \"   Sampling:\\n\"\n",
    "            output_str += \"    \" + str(r_hist) + \"\\n\"\n",
    "\n",
    "            l = []\n",
    "            for j in range(3):\n",
    "                l.append(seqs[j,:10,:,:,:])\n",
    "                l.append(cnn_decoder_out[j,:,:,:,:].data.cpu())\n",
    "                l.append(seqs[j,10:,:,:,:])\n",
    "            samples = torch.cat(l)\n",
    "            torchvision.utils.save_image(samples,\n",
    "                                         train_out_dir + \"/{0:0>5}iter.png\".format(i), nrow=10)\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            #evaluate on the test set\n",
    "            test_mse, test_bce, pnsr, fr_mse = tester.run_test(cnn_encoder,cnn_decoder, lstm_encoder, lstm_decoder)\n",
    "            if i == 0:\n",
    "                min_test_bce = test_bce\n",
    "\n",
    "            output_str += \"-------------------------------------------\\n\"\n",
    "            output_str += \"   Test BCE:      {0:.6f}\\n\".format(test_bce)\n",
    "            output_str += \"   Test MSE:      {0:.6f}\\n\".format(test_mse)\n",
    "            output_str += \"-------------------------------------------\\n\"\n",
    "            output_str += \"   Test PNSR:\\n\"\n",
    "            output_str += \"    \" + str(pnsr.data.cpu()) + \"\\n\"\n",
    "            output_str += \"   Frame mse:\\n\"\n",
    "            output_str += \"    \" + str(fr_mse.data.cpu()) + \"\\n\"\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            output_str += \"\\n=================================================\\n\\n\"\n",
    "            print(output_str)\n",
    "            with open(log_file, \"a\") as lf:\n",
    "                lf.write(output_str)\n",
    "\n",
    "        if i > 35000 and (i % 400 == 0 or test_bce < min_test_bce):\n",
    "            min_test_bce = min(min_test_bce, test_bce)\n",
    "            save_model(i, total_loss.item(), test_bce)\n",
    "\n",
    "        #--------------------\n",
    "\n",
    "        i += 1\n",
    "    if i >= 60000:\n",
    "        exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
